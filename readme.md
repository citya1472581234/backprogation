# backpropagation
![](https://github.com/citya1472581234/backprogation/blob/master/DNN/picture/1.PNG?raw=true)


![](https://github.com/citya1472581234/backprogation/blob/master/DNN/picture/2.PNG?raw=true)

3.Dropout
* 訓練時以概率 P，保留神經元，測試時所有神經元都參與。

4.conv
* dout 對應區域，和卷積和W反轉卷積，等效於dout的每一個值和卷積和相乘，然後對dx對應區進行疊加

![](https://i.imgur.com/wmBts1R.png)


5.optimizer
* 各種 optimizer 方法 https://blog.csdn.net/u014595019/article/details/52989301 
* adam RMS momentum 比較 https://blog.csdn.net/willduan1/article/details/78070086
* Nesterov accelerated gradient (NAG) https://blog.csdn.net/tsyccnh/article/details/76673073
![](https://i.imgur.com/7avHxqq.png)

6.
![](https://github.com/citya1472581234/backprogation/blob/master/DNN/picture/3.PNG?raw=true)
![](https://github.com/citya1472581234/backprogation/blob/master/DNN/picture/4.PNG?raw=true)
